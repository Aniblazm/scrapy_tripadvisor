# -*- coding: utf-8 -*-
import scrapy
# from scrapy.contrib.spidermiddleware.httperror import HttpError
import random
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from scrapy.spidermiddlewares.httperror import HttpError
from twisted.internet.error import DNSLookupError
from twisted.internet.error import TimeoutError
from datetime import datetime



class TripadvisorMadrid1Spider(scrapy.Spider):
    name = 'tripadvisor_madrid_1'
    allowed_domains = ['tripadvisor.com']
    start_urls = ['https://www.tripadvisor.com/Hotels-g187514-Madrid-Hotels.html/']

    rules = (
        Rule(LinkExtractor(),
             callback='parse_page',
             # hook to be called when this Rule generates a Request
             process_request='add_errback')
    )

    # this is just to no retry errors for this example spider
    custom_settings = {
        'RETRY_ENABLED': False
    }

    # method to be called for each Request generated by the Rules above,
    # here, adding an errback to catch all sorts of errors
    def add_errback(self, request):
        self.logger.debug("add_errback: patching %r" % request)

        # this is a hack to trigger a DNS error randomly
        rn = random.randint(0, 2)
        if rn == 1:
            newurl = request.url.replace('httpbin.org', 'httpbin.organisation')
            self.logger.debug("add_errback: patching url to %s" % newurl)
            return request.replace(url=newurl,
                                   errback=self.errback_httpbin)

        # this is the general case: adding errback to all requests
        return request.replace(errback=self.errback_httpbin)

    def parse_page(self, response):
        self.logger.info("parse_page: %r" % response)

    def errback_httpbin(self, failure):
        # log all errback failures,
        # in case you want to do something special for some errors,
        # you may need the failure's type
        self.logger.error(repr(failure))

        if failure.check(HttpError):
            # you can get the response
            response = failure.value.response
            self.logger.error('HttpError on %s', response.url)

        elif failure.check(DNSLookupError):
            # this is the original request
            request = failure.request
            self.logger.error('DNSLookupError on %s', request.url)

        elif failure.check(TimeoutError):
            request = failure.request
            self.logger.error('TimeoutError on %s', request.url)

    def parse(self, response):
        # Extract data using xpath
        hotel_name = response.xpath('//div[@class="prw_rup prw_meta_hsx_listing_name listing-title"]/div[@class="listing_title"]/a[@class="property_title prominent "]/text()').extract()
        review_count = response.xpath('//a[@class="review_count"]/text()').extract()
        review_avg = response.css('[class~="ui_bubble_rating"]::attr(alt)').extract()

        timestamp = time.time()
        inserted_at = datetime.fromtimestamp(timestamp)

        row_data = zip(hotel_name, review_count, review_avg,inserted_at)



        # Making extracted data row wise
        for item in row_data:
            # create a dictionary to store the scraped info
            scraped_info = {
                # key:value
                'page': response.url,
                'hotel_name': item[0],
            # item[0] means product in the list and so on, index tells what value to assign
                'review_count': item[1],
                'review_avg': item[2],
                'inserted_at': inserted_at
            }

            # yield or give the scraped info to scrapy
            yield scraped_info

            NEXT_PAGE_SELECTOR = '[class="nav next taLnk ui_button primary"]::attr(href)'
            next_page = response.css(NEXT_PAGE_SELECTOR).extract_first()

            if next_page:
                yield scrapy.Request(
                    response.urljoin(next_page),
                    callback=self.parse)
